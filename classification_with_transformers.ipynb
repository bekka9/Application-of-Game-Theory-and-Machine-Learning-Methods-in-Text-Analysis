{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom d2l import torch as d2l\nfrom transformers import BertTokenizer\n\n\nclass MultiHeadAttention(d2l.Module): \n    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n        super().__init__()\n        self.num_heads = num_heads\n        self.attention = d2l.DotProductAttention(dropout)\n        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n\n    def forward(self, queries, keys, values, valid_lens):\n        queries = self.transpose_qkv(self.W_q(queries))\n        keys = self.transpose_qkv(self.W_k(keys))\n        values = self.transpose_qkv(self.W_v(values))\n\n        if valid_lens is not None:\n            valid_lens = torch.repeat_interleave(\n                valid_lens, repeats=self.num_heads, dim=0)\n\n        output = self.attention(queries, keys, values, valid_lens)\n        output_concat = self.transpose_output(output)\n        return self.W_o(output_concat)\n\n    def transpose_qkv(self, X):\n        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n        X = X.permute(0, 2, 1, 3)\n        return X.reshape(-1, X.shape[2], X.shape[3])\n\n    def transpose_output(self, X):\n        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n        X = X.permute(0, 2, 1, 3)\n        return X.reshape(X.shape[0], X.shape[1], -1)\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, num_hiddens, dropout, max_len=1000):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.P = torch.zeros((1, max_len, num_hiddens))\n        X = torch.arange(max_len, dtype=torch.float32).reshape(\n            -1, 1) / torch.pow(10000, torch.arange(\n            0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n        self.P[:, :, 0::2] = torch.sin(X)\n        self.P[:, :, 1::2] = torch.cos(X)\n\n    def forward(self, X):\n        X = X + self.P[:, :X.shape[1], :].to(X.device)\n        return self.dropout(X)\n\n\nclass PositionWiseFFN(nn.Module):\n    def __init__(self, ffn_num_hiddens, ffn_num_outputs):\n        super().__init__()\n        self.dense1 = nn.LazyLinear(ffn_num_hiddens)\n        self.relu = nn.ReLU()\n        self.dense2 = nn.LazyLinear(ffn_num_outputs)\n\n    def forward(self, X):\n        return self.dense2(self.relu(self.dense1(X)))\n\nclass AddNorm(nn.Module): \n    def __init__(self, norm_shape, dropout):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.ln = nn.LayerNorm(norm_shape)\n\n    def forward(self, X, Y):\n        return self.ln(self.dropout(Y) + X)\n\nclass TransformerEncoderBlock(nn.Module): \n    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout,\n                 use_bias=False):\n        super().__init__()\n        self.attention = MultiHeadAttention(num_hiddens, num_heads,\n                                                dropout, use_bias)\n        self.addnorm1 = AddNorm(num_hiddens, dropout)\n        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n        self.addnorm2 = AddNorm(num_hiddens, dropout)\n\n    def forward(self, X, valid_lens):\n        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n        return self.addnorm2(Y, self.ffn(Y))\n\n\nclass TransformerEncoder(d2l.Encoder): \n    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n                 num_heads, num_blks, dropout, use_bias=False):\n        super().__init__()\n        self.num_hiddens = num_hiddens\n        self.embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n        self.blks = nn.Sequential()\n        for i in range(num_blks):\n            self.blks.add_module(\"block\"+str(i), TransformerEncoderBlock(\n                num_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))\n\n    def forward(self, X, valid_lens):\n        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))\n        for blk in self.blks:\n            X = blk(X, valid_lens)\n        return X\n\n\nclass TextClassifier(nn.Module):\n    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n                 num_blks, num_classes, dropout):\n        super().__init__()\n        self.encoder = TransformerEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\n                                          num_heads, num_blks, dropout)\n        self.fc = nn.Linear(num_hiddens, num_classes)\n\n    def forward(self, X, valid_lens):\n        X = self.encoder(X, valid_lens)\n        X = X[:, 0, :]\n        return self.fc(X)\n\n\nclass TextDataset(Dataset):\n    def __init__(self, data_dir, tokenizer, max_len):\n        self.data_dir = data_dir\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.texts, self.labels = self.load_data()\n\n    def load_data(self):\n        texts = []\n        labels = []\n        label_map = {'A1': 0, 'A2': 1, 'B1': 2, 'B2': 3, 'C1': 4}\n        for folder in os.listdir(self.data_dir):\n            folder_path = os.path.join(self.data_dir, folder)\n            if os.path.isdir(folder_path):\n                label = label_map[folder]\n                for file_name in os.listdir(folder_path):\n                    if file_name.endswith('.txt'):\n                        file_path = os.path.join(folder_path, file_name)\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            text = f.read()\n                            texts.append(text)\n                            labels.append(label)\n        return texts, labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n        return encoding['input_ids'].squeeze(0), encoding['attention_mask'].squeeze(0), label\n\n\nvocab_size = 30522  # Размер словаря (BERT)\nnum_hiddens = 768\nffn_num_hiddens = 3072\nnum_heads = 12\nnum_blks = 12\nnum_classes = 5\ndropout = 0.1\nmax_len = 512\nbatch_size = 16\nnum_epochs = 10\nlearning_rate = 5e-5\n\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n\ntrain_dataset = TextDataset('path_to_train_data', tokenizer, max_len)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n\nmodel = TextClassifier(vocab_size, num_hiddens, ffn_num_hiddens, num_heads, num_blks, num_classes, dropout)\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for input_ids, attention_mask, labels in train_loader:\n        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}
